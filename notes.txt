Extensible Universal Message Processor

Notes, in reverse chronological order.

++ 2009-06-04

The storage layer must be entirely ignorant of how messages flow into queues, and out again.  This is handled by the engine.  What the storage layer does is store messages, provide access to them, and delete them.  In our current design, the head of the queue is stable insofar as the only party removing messages is the engine itself.

Access to messages is thus synchronous, i.e. the engine polls the storage layer when it needs messages.  This means that we can browse queues using an index that iterates from the head of the queue, i.e. 0 = oldest message, 1 = second oldest, etc.  We need methods like this:

* message_fetch (message, index) - fetch message object by index
* message_update (message) - update message from object
* message_delete (message) - delete message

Which are the same as we previously sketched, only the message_create method having been changed into queue_post.

Perhaps the best identifier for a message is a queue-unique id, rather than the index.  Thus the engine can manipulate messages independently.  In storage layers where the real message is large (e.g. a disk file), the message object acts as a reference.  In storage layers where the real message is small, the message object would hold the denormalized body data.

++ 2009-06-03

+++ Commit 231aeeb: Making the queue API work nicely

Having prototyped the queue API, it looks clumsy.  We'll make these improvements:

* Make the create and fetch methods work like constructors, returning a reference to a newly created object, if successful.
* Use link counting on the xump_queue object, so we can use unlink instead of destroy.  This is nice because if we don't use new, we should not use destroy either.
* Scrap the update method, since this does nothing useful.  We can add it back if needed but for now it looks like queue properties are either established at creation time, or read-only.

So while the portal request API still uses the RESTful model, the xump_queue class layers a more fluid object-oriented API on top of that.  This is what the old calling code looked like (from the xump.icl selftest method):

[[code]
xump_queue_t
    *queue;

queue = xump_queue_new (store, "Test queue");
xump_queue_create (queue);
xump_queue_fetch  (queue);
xump_queue_update (queue);
xump_queue_delete (queue);
xump_queue_destroy (&queue);
[[/code]]

It's clear, but too different from the normal way of working with iCL objects.  Having new and create, and delete and destroy is confusing.  iCL lets me mark the new and destroy methods as private, so these are not exported to callers.  The revised API looks more elegant:

[[code]]
xump_queue_t
    *queue = NULL;
//  Create acts as a constructor
queue = xump_queue_create (store, "Test queue");
//  Unlink is an soft destructor
xump_queue_unlink (&queue);
//  Fetch also acts as a constructor
queue = xump_queue_fetch (store, "Test queue");
//  Delete acts as a hard destructor
xump_queue_delete (&queue);
[[/code]]

+++ Commit cee4083: Implemented queue_post method

I'm not sure that the message API I sketched yesterday is the right one.  In any case, for sticking new messages on queues, it's not right.  The model that feels right is:

* queue_post (queue, message) - post message to queue

Which fits the RESTful pattern and makes this a method of the storage layer's queue class, which is accurate.  The queue is the class that knows how to organize its messages.

Here is the test case for posting messages to a queue:

[[code]]
//  Create a queue and post messages to it
queue = xump_queue_create (store, NULL);
message = xump_message_post (queue, "address1", "abc", 4);
xump_message_unlink (&message);
message = xump_message_post (queue, "address2", "def", 4);
xump_message_unlink (&message);
xump_queue_unlink (&queue);
[[/code]]

The store_ram implementation is pretty trivial, it'll be more fun making a file system storage layer.  Several ways of doing this:

* One file per queue, with one message per line.  For text messages, especially if messages are never deleted.
* One directory per queue, with one message per file.  For large messages, especially for file-transfer type work.  Meta data can be in separate files.
* One file per store, with queues and messages indexed into the file.  Easy management of stores, and very fast if the indexes are properly designed.

Using a database like SQLite would also be fun: one table per queue, one row per message.  This would probably be 10x slower than using a custom indexed file.

++ 2009-06-02

+++ Commit 7e30756: Constructing the portal queue API.

To make it simple, the engine has a single store instance which is just the 'store' property.  This is not a full design.  We'd want to make at least one store instance per backend, but in fact allow the caller to create an arbitrary number of store instances, referenced by name.  We'll add that later.

How this works now - this is from the xump.icl selftest method:

[[code]]
//  Create an engine instance
xump_t
    *xump;
xump = xump_new ();

//  Create a queue object within single store
xump_queue_t
    *queue;
queue = xump_queue_new (xump_store (xump), "Test queue");

//  Check that every methods fails properly
assert (xump_queue_create (queue) == -1);
assert (xump_queue_fetch (queue) == -1);
assert (xump_queue_update (queue) == -1);
assert (xump_queue_delete (queue) == -1);

//  Destroy queue object and engine instance
xump_queue_destroy (&queue);
xump_destroy (&xump);
[[/code]]

Running the create method a billion times takes around 7.5 seconds on a 3GHz core.  So, about 130M requests/second, 7.5 nanoseconds per request.  No bottleneck here, yet, synchronous portals are fast.

+++ Commit 5f06b01: Engine now supports multiple store registration/lookup

Added two methods to the engine, which let the caller register and lookup stores.  It's more sensible that the caller creates the store portal, since it knows what extensions it is using.  The API for creating and registering a new store is short and sweet:

[[code]]
xump_register_store (xump, xump_store_ram__xump_store_new (NULL, storenamegoeshere));
[[/code]]

I called the methods 'register_store' and 'lookup_store' because the object_verb form I'd usually prefer ('store_register' and 'store_lookup') suggest that these methods are in the store class, which they are not.

+++ Commit 9f9a1c2: Improved engine store method names

Actually, on second thoughts, it's even nicer to make the store list look like a property of engine, and use the same style as we use for opaque object properties, namely "set_property" and "property".  This gives us a readable:

[[code]]
xump_set_store (xump, xump_store_ram__xump_store_new (NULL, "RAM1"));
xump_set_store (xump, xump_store_ram__xump_store_new (NULL, "RAM2"));
assert (xump_store (xump, "RAM1") != NULL);
assert (xump_store (xump, "RAM0") == NULL);
[[/code]]

++ 2009-06-02

This single threaded version will let us prototype the APIs.  For now, the APIs will be synchronous, that is, the caller requests something, and gets a response when the work has been done.  The called object and the calling object occupy the same thread.

Let's explore the queue-message-field API further.  We define this API as portal requests in the xump_store.icl class.  Roughly:

* queue_create (&queue, name) - create queue in store
* queue_fetch (&queue, name) - fetch queue object
* queue_update (queue) - update queue from object
* queue_delete (queue) - delete queue
* message_create (&message, queue, address, bucket) - create message in queue
* message_fetch (&message, index) - fetch message object by index
* message_update (message) - update message from object
* message_delete (message) - delete message
* field_create (&field, message, name, value) - create field in message
* field_fetch (&field, message, name) - fetch field object
* field_update (field) - update field from object
* field_delete (field) - delete field

This mirrors a RESTful model, in which a remote resource (in this case, on the other side of the API call) is managed by doing create/retrieve/update/delete on a representation of that resource (in this case, an object).  In a classic OO model, the resource and the object would be the same thing.  Here, they are not.

The reasons for this not-quite-OO design:

* It lets us define resources that have different life cycles than objects, which we need to define persistent resources (that remain alive when the application stops).
* It lets us do concurrent access to shared resources in a safe fashion.
* It lets us hide the implementation of the resources from the calling application.

The word "retrieve" is annoying to type, so I'll use "fetch" instead.  The more often we use a word, the shorter and simpler it should be.

++ 2009-06-01

The xump.icl class provides engine instances.  These isolate some stores, not others.  A RAM-based store sits inside an engine instance.  A disk-based store sits outside the engine instance and can be accessed from any instance.  At this stage we don't do any kind of access control.  So our resource hierarchy changes a little:

[[code]]
Engine
+-- Store
    +-- Queue
        +-- Message
            +-- Field
[[/code]]

Starting on the basic framework for the store portal and plugins.  We have these classes:

* xump.icl - engine class
* xump_store.icl - store portal
* xump_store_ram.icl - RAM based store stub that says hello

All single threaded, so no SMT, no multicore.  That makes the classes simpler, but for any kind of performance, or when using disk-based persistence, we'll need multithreading.  To be designed later.

What the stub RAM store looks like (trimming comments):

[[code]]
<?xml?>
<class
    name      = "xump_store_ram"
    comment   = "Xump RAM store back-end"
    script    = "icl_gen"
    license   = "gpl"
    opaque    = "1"
    >
<inherit class = "xump_store_back" />
<method name = "announce">
    icl_console_print ("I: initializing RAM-based storage layer");
</method>
</class>
[[/code]]

And this works:

[[code]]
$ boom build
boom I: [Xump]: Generating files...
boom I: [Xump]: Building Xump...
$ boom test
boom I: [Xump]: Running regression tests...
[[/code]]

This is the first running code.  Not much, but it registers and calls a plugin.  Committed as change 014a172.

++ 2009-05-30

I'm analysing the storage layer, which holds queues and messages in different ways (RAM, file system, SQL database, and so on).  My goal is to make a minimal storage layer using the iPR portal system, with two or three different implementations.  Then, to test these and get performance measurements.  The design for Xump is very roughly this:

[[code]]
+--------------------------------+
|        Application layer       |
+--------------------------------+
|          Routing layer         |
+--------------------------------+
|          Storage layer         |
+----------+----------+----------+
| Back-end | Back-end | Back-end |
+----------+----------+----------+
[[/code]]

Making it simpler... what can we ignore for now?  Our use case for Xump (which is a social messaging layer above Wikidot) calls for very high volumes of quite small text messages (80-160 characters).  We would expect thousands of queues, each containing thousands of messages.  We'd want these to be persistent and archived, with messages being held on their queues for as long as the queues existed.

We can ignore messages with multipart contents.  This is an advanced use case and arguably it is not a good one, since it creates messages that are complex for applications to work with.  If my application produces multipart messages, that makes it hard for others to talk to me.  So a framework that allows this may actually be helping me to shoot myself in the foot.

For high volumes of small messages, copying is not a major concern.  We avoid copying by manipulating references, but that is not free.  So, we will for now copy data.  Later, and for larger messages, we'll see about normalizing data so that copying is reduced when the same message is sent to many consumers.

Every single message has an address, which is used heavily, so we will make this a dedicated property of the class.  A message also has content body, content length, and a table of name/value fields.

Let's build the storage system around queues.  That is, a store holds a number of queues, which hold a number of messages.  We can interrogate the store for queues and messages, and it will return us memory objects that we can work with.

The iPR portal system provides a virtual class interface.  Our portal class will be xump_store.icl.  This would have implementations (called "portal back-ends") like xump_store_fs.icl (file system), xump_store_ram.icl, xump_store_sql.icl, and so on.  Each portal back-end instance is equal to a store instance, and implements the portal methods.  Which we can break into:

* Creating and working with queues in a particular store.
* Creating and working with messages in a particular queue (in its store).

The queues and messages that a store manages are opaque, and invisible as such to the storage layer.  To work with a queue, the caller asks the back-end to provide a queue object.  It can execute methods on this object (some of which may affect the stored queue).  It then destroys the queue object.  Messages are handled in the same way.

We will use the create-retrieve-update-delete pattern for working with queues and messages.  To some extent this mirrors the RESTful pattern for working with remote resources.  In this case, the remote resources sit in a store that is opaque to the caller, which accesses it via the portal interface.

To repeat in a different way: when the caller does xump_queue_destroy(), this destroys a queue object but does not destroy the queue held in the store.

The queue and message objects that the store provides to the calling layer are private, not locked, and not shared (they do not support reference counting).

The simplest workable identifiers for queues are names.  It makes sense that a store can auto-name private queues.  For messages, we can use sequence numbers.

The xump_store.icl portal class needs these methods for working with queues:

* Create (named or unnamed) queue in store, with given properties.
* Retrieve queue from store by name.
* Update queue with given properties.
* Delete queue from store.

And for working with messages:

* Create new message in queue, with given content and address.
* Retrieve message from queue (there are various patterns here).
* Update a message with given properties.
* Delete message from queue.

And finally, to manipulate fields on messages:

* Create new field on message, with given name and value.
* Retrieve field from message, by name.
* Update field value.
* Delete field from message.

We then need classes to hold queue, message and field references:

* xump_queue.icl, which refers to a queue in a store.
* xump_message.icl, which refers to a message in a queue.
* xump_field.icl, which refers to a field in a message.

So our resource hierarchy looks like this:

[[code]]
Store
+-- Queue
    +-- Message
        +-- Field
[[/code]]

++ 2009-05-25

Have added opaque classes to iCL, and released an upgrade of OpenAMQ that uses these in a few places.  Opaque classes work fine except for one common case, when they're used in invasive containers.  We could use the neutral containers but they are a lot of extra work in use.

Decision: all public classes will be opaque and when we need to make lists or tables of them, we'll use the ipr_looseref_list and ipr_hash classes.

So, API classes have this general interface:

objref = myclass_new (arguments);
objcopy = myclass_link (objref);
myclass_some_method (objref);
myclass_unlink (&objcopy);

* All classes are reference counted.
* All classes are synchronous.
* All classes are threadsafe.
* All classes are opaque.

++ 2009-05-17

Xump is meant to become an embeddable messaging engine that solves two recurrent problems (we see in OpenAMQ and Zyre), namely routing and storage.  It should do for messaging products what game engines do for video games.  Standard tactic for software engineering: make 1 and 2 by hand, then solve N with a general solution.

The design of Xump is a mix of ideas collected over the last few years.  The engine manages a set of FIFO //queues//.  //Messages// flow through one or more queues in a directed network of any complexity.  Everything is a queue so the whole network is asynchronous.  The calling application (that is, the product into which we embed the Xump engine) constructs this network dynamically at runtime, though the network may also be persistent.  The lifecycle of all resources is visible to, and managed by, the calling application.

Queues pull messages from other queues via //selectors//, which specify static //matching// and //filtering// operations. Selectors are like stored search criteria that act on the head of the queue, eating up or otherwise operating on messages.  A selector reads from one queue, and writes to another queue or set of queues.  Matching is a O(logn) set operation, in which Xump compares a message address to the set of selectors that read on a queue.  Filtering is a O(n2) operation in which Xump compares one message to one selector.  In theory we match first, and filter second.

Selector match and filter operations are: Move, Copy, and Delete.  These work as you would expect.   Here is how two applications divide the flow of messages on a queue (the Wolfpack pattern for workload distribution):

[[code]]
Queue1
    -> Selector MOVE
        -> Queue2
    -> Selector MOVE
        -> Queue3
[[/code]]

Here is how two applications each get the full flow of messages (the Parrot pattern for information distribution):

[[code]]
Queue1
    -> Selector COPY
        -> Queue2
    -> Selector COPY
        -> Queue3
[[/code]]

A mix of selector operations is valid.  Here is how two applications divide the work on a queue, while a third gets a copy of every message:

[[code]]
Queue
    -> Selector MOVE
        -> Recipient
    -> Selector MOVE
        -> Recipient
    -> Selector COPY
        -> Recipient
[[/code]]

The implementation of selectors is extensible, via a virtual class interface, and the intention is that message processing applications that use Xump would provide their own implementations for the xump_select class.

To read messages off a queue, applications (that is, messaging servers which use Xump as their embedded engine) open and close "cursors" on queues.  Xump cursors are analogous to the SQL cursor concept.  The main difference is that when a cursor is open, messages are sent asynchronously to the application.

Applications use Xump by:

* Constructing a directed network of queues and selectors;
* Setting up cursors.
* Publishing messages to queues.
* Delivering messages received from cursors.

The persistence layer is extensible, via a virtual class interface, with the intention being that applications which embed Xump would provide their own storage layer implementations.  Persistence happens at the queue level, so it is possible to e.g. provide persistent subscriptions for some clients, and transient ones for others.

Delivery semantics (e.g. message acknowledgement) affect selectors insofar as selectors may want acknowledgment for their operations, and effectively block until they get it.  This would cause the queue to build up, typical for Wolfpack.  Without this, typical for Parrot, queues will process messages at full speed and mainly remain empty (unless sheer processing costs caused backlogs).
